## 编码规则

### bit&byte
在计算机中所有信息都是以二进制位`0`和`1`来存储的。8个bit位是一个字节，计算机对数据的读取是按照一个字节的大小来读取识别的，那么面对全世界这么多语言，计算机是如何知道当前字节表示的是什么意思呢？
今天就来了解下字符在计算机中的编码方式，其中会涉及到`ASCII` `unicode` `UTF-8`等。

我们回顾一下，ASCII表示的范围：

![img](../assets/app-encoding.ascii.png)

### ASCII
首先
首先是美国人制定了一套二进制和英文字母的映射关系，比如大写字母`A`对应的是二进制`01000001`也就是十进制`65`。
ASCII起初只规定了127个字符，完全满足英文使用，但是其他欧洲国家比如法语、葡萄牙语等无法用127个字符表示完全，
所以他们就把第一个空闲0使用了起来。因此欧洲国家用一个字节(8位可表示256个字符)也能实现。

### unicode
但是其他语言，比如中文、日文，256个字符肯定完全没法表示。所以就出现了一个全球统一的编码规则叫`unicode`，它的作用是把各个语言的字符映射为二进制。
比如汉字`中`对应的unicode十六进制是`4e2d`转换为十进制是`20013`转换为二进制是`100111000100101`，二进制有15位，需要至少2个字节表示。其他更大的符号，需要更多的字节数表示。
所以这就出现了一个问题，计算机按照字节来读取数据时，如何知道当前字节就是要表示一个字符还是连续的三个字节表示一个字符？如果按照`unicode`的方式统一用三个或者四个字节表示一个字符，那英文字符前面2个/3个字符就全是0，存储空间是极大的浪费。

### UTF-8
UTF-8的诞生，它就是`unicode`的实现方式之一，也就是说由它来告诉计算机如何读取`unicode`。它的特点是变长编码，可以使用1-4个字节表示一个字符。

UTF-8的规则很简单，就是使用控制码+字符码组成，控制码就是告诉计算机当前是单字节还是多字节，字符码就是对应的`unicode`。

规则1：对于单字节的字符，8个bit位中高位必须以`0`开头，这完全等同于127位最初的`ASCII`码。比如大写字母`A`，对于的二进制为`01000001`
规则2：对于需要n个字节来表示的字符中，以三字节字符来举例，第一个字节是`1110xxxx`前面的`1110`为控制码表示当前字符是三字节的，后面两个字节都是`10xxxxxx`。
